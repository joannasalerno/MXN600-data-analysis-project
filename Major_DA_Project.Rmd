---
title: "MXN600 Major Data Anaysis Project"
author: "Joanna Salerno, Pavan Asopa, Sevin Nejadi, Fernanda Martins Giuriati"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The credit risk models our lending start up company uses are of the utmost importance to the functioning and ultimate success of the company. As we were recently acquired by a regional Australian bank, the success of our company also impacts the success of the bank. Recently, some of the bank's senior financial analysts have raised concerns about the credit risk models we have been using. They reviewed our models' performance benchmarks and feel as though our models are not suitable for use in a setting in which they are subject to strict regulatory requirements.

Thus, we have been tasked by the bank's management to rebuild our credit risk model from the ground up. As per management, our main objective is to use information known at the time of a loan application to build a model that predicts loan default. We will follow a standard statistical analysis process, which will be guided by the following questions:

1. How does this new model perform compared to the one used previously? How can it be expected to perform on new loan applications?
2. What are the important variables in this model and how do they compare to variables that are traditionally important for predicting credit risk in the banking sector?

Furthermore, management has consulted with an expert statistician, who has suggested we also account for variation in trends that may exist either between different jurisdictions or over time. The following questions will guide this second part of our analysis:

3. Can accounting for this variation (e.g., state/zip-code and time) improve performance benchmarks?
4. Are there any surprising differences in variables that are important for predicting credit risk?
5. Does credit risk change over time or between states? This is not something the bank has previously
investigated and results may inform modified loan policies in the future.

This report will document our entire analysis process, beginning with data exploration and cleaning, to model building and interpretations of our results.

# Setup

We will first load in the required libraries for our data exploration and analysis process.

```{r load_libraries, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(MASS)
library(GGally)
library(lme4)
library(lmerTest)
library(DHARMa)
library(pROC)
library(dplyr)
```

Next, we will load in our datasets. We have a total of 4:

(1) A training dataset that we will use to build and train our model
(2) A test dataset that we will use to test the fit of our model(s)
(3) A validation dataset that we will use to assess the performance of our model(s)
(4) An extended dataset that includes the necessary variables for us to account for variation such as location and time

```{r load_data}
train_data <- read.csv("benchmark_training_loan_data.csv")
test_data <- read.csv("benchmark_testing_loan_data.csv")
val_data <- read.csv("benchmark_validation_loan_data.csv")
extended <- read.csv("extendend_version_loan_data.csv")
```

# Exploratory Analysis

We will begin by exploring the available data to understand how each variable is distributed and to identify any potential data quality issues. We will also investigate the relationships between the different variables to see whether any variables are highly correlated with one another.

Note: For this exploration portion of our analysis, we will be using the training dataset.

We will first explore the training dataset to understand its structure and the variables it is comprised of.

```{r head_data}
head(train_data)
```

At first glance of the first 6 rows of this dataset, we notice there is a value of n/a in the employment length column. There are also a few zero values in a few columns. This will prompt us to further explore the data for any true missing values.

```{r data_dim}
dim(train_data)
```

The training dataset contains a total of 23,052 observations of 19 variables.

```{r data_structure}
str(train_data)
```

Upon further investigation of the structure of the training data, we can see that 14 of the 19 variables are currently of numeric type, and the remaining 5 variables are characters.

We will now remove variable X, which is the number of each record, from all datasets, as it is not necessayr for this analysis.

```{r remove_X_from_datasets}
train_data <- train_data[, -1]
val_data <- val_data[, -1]
test_data <- test_data[, -1]
extended <- extended[, -1]
```

We will now further investigate each variable to determine whether there is any missing data or outliers.

```{r data_summary}
summary(train_data)
```

Included above is the numeric distributions of each of the included variables. Based on the above summary, it appears as though there may exist one or multiple outliers in a few variables: annual income, inquiries in the last 6 months, open accounts, revolving balances, and total accounts. However, we will need to further investigate the distribution of all variables to better visualize this to determine whether these actually appear to be outliers.

Before producing some exploratory plots, we will briefly explore the data to see whether there are missing values for us to handle.

```{r sum_na}
colSums(is.na(train_data))
```

Based on the above output, it appears as though this dataset does not contain any missing data in the form of NA values. Now, we will explore the n/a values present in the columns which include string data.

```{r string_na_sum}
sum(train_data == 'n/a')
```

```{r string_na_rows}
na_rows <- train_data %>% filter_all(any_vars(. %in% c('n/a')))
head(na_rows)
```

Based on the above output, there are a total of 591 n/a (string) in this dataset. These all appear to be from the employment length column. We will move forward and assume that this is not truly missing data, but rather, means that the applicant is not currently employed. We will further examine the ditribution of each variable by creating exploratory plots, and this will further clarify the meaning of some of the values contained within each column of the dataset.

## Exploratory Plots

Before we can create exploratory plots, there are a few variables we will need to convert to factors. This will help us in creating our visualizations as well as conducting our analyses, so we can consider the selected variables as factors with different levels rather than simply strings.

```{r convert_vars_to_factors_train}
train_data$term <- as.factor(train_data$term)
train_data$emp_length <- as.factor(train_data$emp_length)
train_data$home_ownership <- as.factor(train_data$home_ownership)
train_data$verification_status <- as.factor(train_data$verification_status)
train_data$purpose <- as.factor(train_data$purpose)
train_data$repay_fail <- as.factor(train_data$repay_fail)
```

```{r convert_vars_to_factors_val}
val_data$term <- as.factor(val_data$term)
val_data$emp_length <- as.factor(val_data$emp_length)
val_data$home_ownership <- as.factor(val_data$home_ownership)
val_data$verification_status <- as.factor(val_data$verification_status)
val_data$purpose <- as.factor(val_data$purpose)
val_data$repay_fail <- as.factor(val_data$repay_fail)
```

```{r check_imbalance_data}
plot(train_data$repay_fail)
```

Despite the significant class imbalance in the dataset, logistic regression still maintains its probabilistic nature. However, it's important to note that imbalanced data can affect the model's predictive performance, potentially leading to a bias towards the majority class.

Now, we will create some histograms to visualize the distributions of some of the string/character variables in this dataset.

```{r histograms}
p1 <- ggplot(data = train_data, aes(int_rate, fill = repay_fail)) +
  geom_histogram(binwidth=5)

p2 <- ggplot(data = train_data, mapping = aes(x = verification_status, y = annual_inc)) +
  geom_boxplot() +
  theme_bw()

p3 <- ggplot(data = train_data, mapping = aes(x = repay_fail, y = loan_amnt)) +
  geom_boxplot() +
  theme_bw()

p4 <- ggplot(data = train_data, mapping = aes(x = repay_fail, y = annual_inc)) +
  geom_boxplot() +
  theme_bw()

p5 <- ggplot(data = train_data, mapping = aes(x = annual_inc, y = loan_amnt)) +
  geom_point() +
  theme_bw()

ggarrange(p1, p2)
ggarrange(p3, p4)
ggarrange(p5)
```
Explanations: 

Plot1 is a histogram that displays the distribution of interest rates in the dataset. The bars in the histogram are filled based on whether the loan was repaid successfully or not, where fill colours represent the two categories 0 for no failure and 1 for failure of loan repayment. Based on the plot, a trend can be seen that as the interest rate increases so does the chances of loan repayment failures.

Plot2 is a boxplot that shows the distribution of annual incomes based on different verification statuses. Each box represent a category of verification status. Based on the plot, we can see that we have similar sized boxes for all three categories which suggests that the median income for applicants in these categories is relatively consistent. However, we can see the presence of outlier in all three verification_status categories indicating that there are individuals with very high income within each group.

Plot3 is a boxplot that compares the loan amounts for both successfull and unsuccessfull loan repayments. Based on the plot, it can be seen that the median loan amount for both categories of loan repayment status are more or less consistent. The presence of outlier in both categories indicates that there are individuals with very high loan amount within each group. 

Plot4 is a boxplot that compares annual income between successful and unsuccessful loan repayments. Based on the plot it can be seen that most individuals that successfully paid off their loans are more or less consistent with the ones who failed with an exception of ouliers in both categories.

Plot5 is scatter plot that displays relationship annual income and loan amount. 

Here we create some distribution plots to have a better visualization of the data:

```{r dist_plots_1}
numeric_vars <- c("loan_amnt", "annual_inc", "dti", "delinq_2yrs", "inq_last_6mths",
                  "open_acc", "pub_rec", "revol_util", "revol_bal", "total_acc",
                  "credit_age_yrs")
par(mfrow=c(3,4))
for (var in numeric_vars) {
  hist(train_data[[var]], main=var, xlab=var)
}
```
The above plot displays the distribution of all numeric variables in the dataset in the form of a histogram. 

```{r dist_plots_2}
ggplot(train_data, aes(x = purpose)) +
  geom_bar() +
  labs(x = "Purpose", y = "Frequency") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The above bar plot represents the frequency of each unique value in the purpose variable. From the plot it can be seen that the most common reasons to take loans are debt_consolidation, credit_card, home_improvement and car.

```{r dist_plots_3}
ggplot(train_data, aes(x = term, y = loan_amnt)) +
  geom_boxplot() +
  labs(x = "Term", y = "Loan Amount")
```

The above plot consists of two side-by-side boxplots, one for each loan term category. Each boxplot provides information about the distribution of loan amounts for its respective term category. Based on the plot, we can see that, on average, loans with 60 months term have higher loan amounts than those with a 36 month term which is an expected behaviour. Also, we can observe some outliers in the 36 months term which indicates that there are a few loan amounts significantly higher than the typical range of loan amounts for that category.

Now, we will create a plot that depicts the correlation between the different numeric variables; it includes both correlation coefficients and visuals of the distribution of each included variable.

```{r ggpairs_plot}
# create ggpairs plot with the specified columns
ggpairs_plot <- ggpairs(train_data[, c("loan_amnt","int_rate", "annual_inc", "dti",
                                      "delinq_2yrs", "inq_last_6mths", "open_acc",
                                      "pub_rec", "revol_util", "revol_bal", "total_acc",
                                      "credit_age_yrs")],
                        progress = FALSE)

#ggpairs_plot
```

Below, we create a correlation matrix of the numeric variables to help us understand the relationship between the different numeric variables.

```{r corr_matrix}
columns <- c("loan_amnt", "annual_inc", "dti", "delinq_2yrs", "inq_last_6mths", "open_acc", "pub_rec", "revol_util", "revol_bal", "total_acc", "credit_age_yrs","int_rate")
corr_matrix <- cor(train_data[columns], method = "pearson")
ggcorr(corr_matrix, type = "lower", hjust = 1, size = 3, color = "grey")
```

*** ADD EXPLANATION

# New Credit Risk Model

The correlation matrix (Heatmap) shows that:

* open_acc and total_acc are strongly positively correlated, so we will keep total_acc and remove open_acc
* revol_util and int_rate are strongly positively correlated, so we will keep int_rate and revol_util will be removed

Based on the [Five Cs of Credit](https://www.investopedia.com/ask/answers/040115/what-most-important-c-five-cs-credit.asp) information and considering Correlation between variales below are the predictors more important to assess the risk whether the creditor will default on a loan:

annual_inc, emp_length, verification_status, dti, credit_age_yrs, home_ownership, total_acc, revol_bal, int_rate, term, purpose, loan_amnt, inq_last_6mths, delinq_2yrs

Now, we'll standardize our numeric variables so that they can be accurately compared:

```{r train_standardisation}
numeric_columns <- sapply(train_data, is.numeric)  # Identify numeric columns

# Scale the numeric columns while keeping column names
train_data[, numeric_columns] <- scale(train_data[, numeric_columns])

train_scaled_data <- train_data %>%
  mutate_if(is.numeric, scale)
```

```{r val_standardisation}
numeric_columns <- sapply(val_data, is.numeric)  # Identify numeric columns

# Scale the numeric columns while keeping column names
val_data[, numeric_columns] <- scale(val_data[, numeric_columns])

val_scaled_data <- val_data %>%
  mutate_if(is.numeric, scale)
```

## Logit Link Function

Fit first logit model:

```{r fit_logit}
fit_logit <- glm(repay_fail ~ int_rate + emp_length + annual_inc + purpose + 
                   inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate,
                 data = train_scaled_data, family = binomial) #18156.43 

 AIC(fit_logit)
```

```{r summary_fit_logit}
summary(fit_logit)
```

int_rate + emp_length + annual_inc + purpose + inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate

```{r fit_logit_resid}
# simulate residuals from the model:
res = simulateResiduals(fit_logit)

# plot observed quantile versus expected quantile to assess distribution fit, and predicted value versus standardised residuals for unmodelled pattern in the residuals
plot(res)
```

```{r train_roc}
# ROC curve on scaled train data 
prob=predict(fit_logit,type=c("response"))
g <- roc(train_scaled_data$repay_fail ~ prob)
AUC <- g$auc
AUC
Gini <- 2*(AUC - 1/2)
Gini
plot(g)
```

```{r val_roc}
# ROC curve on scaled validation data
prob=predict(fit_logit,newdata=val_scaled_data,type=c("response"))
g <- roc(val_scaled_data$repay_fail ~ prob)
AUC <- g$auc
AUC
Gini <- 2*(AUC - 1/2)
Gini
plot(g)
```

## Probit Link Function

```{r fit_probit}
fit_probit <- glm(repay_fail ~ int_rate + emp_length + annual_inc + purpose + inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate,
    data = train_scaled_data, family = binomial("probit"))

AIC(fit_probit)
```

```{r probit_train_roc}
# Probit model: ROC curve on scaled train data 
prob=predict(fit_probit,type=c("response"))
g <- roc(train_scaled_data$repay_fail ~ prob)
AUC <- g$auc
AUC
Gini <- 2*(AUC - 1/2)
Gini
plot(g)
```

```{r probit_val_roc}
# Probit model: ROC curve on scaled validation data 
prob=predict(fit_probit,newdata = val_scaled_data,type=c("response"))
g <- roc(val_scaled_data$repay_fail ~ prob)
AUC <- g$auc
AUC
Gini <- 2*(AUC - 1/2)
Gini
plot(g)
```

## Cloglog Link Function

```{r fit_cloglog}
fit_cloglog <- glm(repay_fail ~ int_rate + emp_length + annual_inc + purpose + inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate,
    data = train_scaled_data, family = binomial("cloglog")) #18156.43

AIC(fit_cloglog)
```

## Calculate odds ratio and CI for the final model of GLM

```{r odds_ratio_GLM}
summary_logit <- summary(fit_logit)

parameter_estimates <- summary_logit$coef[, "Estimate"]
standard_errors <- summary_logit$coef[, "Std. Error"]
p_values <- summary_logit$coef[, "Pr(>|z|)"]

# Calculate the odds ratios by exponentiating the parameter estimate values
odds_ratios <- exp(parameter_estimates)

# Calculate the confidence intervals for the odds ratios
lower_ci <- exp(parameter_estimates - 1.96 * standard_errors)  # 1.96 corresponds to a 95% confidence interval
upper_ci <- exp(parameter_estimates + 1.96 * standard_errors)

model_results <- data.frame(
  Estimate = parameter_estimates,
  OddsRatio = odds_ratios,
  LowerCI = lower_ci,
  UpperCI = upper_ci,
  p_value = p_values
)

knitr::kable(
  model_results,
  col.names = c("Exponentiated Estimate", "Odds Ratio", "Lower Bound (2.5%)", "Upper Bound (97.5%)", "p-value")
)
```

### Interpretations

Listed below are some interpretations of the significant covariates of our final model:

* Intercept is significant
* int_rate: Higher interest rates increase the chances of defaulting
* employment_length == n/a: With an unknown employment length, higher chances of default
* annual_inc: As income increases, chances of defaulting decrease (negative coefficient)
* purposes: medical, moving, other, small business, vacation: All associated with higher chances of defaulting
* inq_last_6mths: More inquiries associated with higher chances of default
* pub_rec: More public records associated with higher chances of default
* revol_bal: Higher credit revolving balances associated with higher chances of default
* revol_util: Utilizing a larger amount of available credit associated with higher chances of default
* term == 60 months: Having a longer repayment period (compared to 36 months) associated with higher chances of default
* int_rate x term == 60 months: Interaction between interest rate and term of 60 months associated with a decreased risk of loan default

## Odds ratio plot for the final model of GLM

```{r odds_ratio_plot_GLM}
plot_data <- data.frame(
  Variable = rownames(model_results),
  OddsRatio = model_results$OddsRatio,
  LowerCI = model_results$LowerCI,
  UpperCI = model_results$UpperCI
)

ggplot(plot_data, aes(x = OddsRatio, xmin = LowerCI, xmax = UpperCI, y = Variable)) +
  geom_point(size = 3, color="red") +
  geom_errorbarh(height = 0.2, color = "blue") +
  xlim(c(0, max(plot_data$UpperCI + 1))) +  
  theme_minimal() +
  labs(x = "Odds Ratio", y = "Variable") +
  ggtitle("Plot of Odds Ratios with CI")
```

*** SOME EXPLANATION

## Calculate accuracy and confusion matrix for the final model of GLM

```{r accuracy_confmatrix_GLM}
fit_logit <- ifelse(prob > 0.5, "1", "0")
fit_logit <- as.factor(fit_logit)

require(caret)
cm <- confusionMatrix(data=fit_logit,
                    reference=val_scaled_data$repay_fail,
                    positive="1")
cm
```

Included above is a confusion matrix, which depicts our model's true and false positive/negative rates. This can be used to better understand how well the model can predict new credit applications in terms of how accurately it classifies new applicants. The confusion matrix shows that out of 7,683 applicants, 6,504 of them will not default and have been accurately classified as so, and 21 of them will default and have been accurately classified as so. However, using our model, 1,138 applicants are falsely classified as defaulters when in fact, they would not have defaulted, and 20 applicants are falsely classified as not defaulting, when they are actually defaulters. 

## Part 1 Discussion

*** DISCUSS OVERALL FINDINGS

# Extended Credit Risk Model

Now, so far, we've been able to address management's concerns regarding the previous credit risk model and have presented a new model that performs much better. We are now going to address the second part of management's concerns, which includes using an extended version of the initial dataset. We will set out to answer whether accounting for variation in trends over jurisdiction or time changes or even improves performance benchmarks. Management has previously never considered whether credit risk changes between different states or over time, so we will investigate this.

We'll first take a look at the structure of the extended dataset.

```{r str_extended}
str(extended)
```

Next, we'll need to once again convert the relevant variables to factors. This time, this will include the new variables of zip_code and addr_state, in addition to the variables converted in the first part of this analysis:

```{r convert_vars_to_factors_ext}
extended$term <- as.factor(extended$term)
extended$emp_length <- as.factor(extended$emp_length)
extended$home_ownership <- as.factor(extended$home_ownership)
extended$verification_status <- as.factor(extended$verification_status)
extended$purpose <- as.factor(extended$purpose)
extended$repay_fail <- as.factor(extended$repay_fail)
extended$zip_code <- as.factor(extended$zip_code)
extended$addr_state <- as.factor(extended$addr_state)
extended$earliest_cr_line <- as.factor(extended$earliest_cr_line)
```

Once again, we need to standardise the numeric variables in the dataset:

```{r standardisation_ext}
numeric_columns <- sapply(extended, is.numeric)  # Identify numeric columns

# Scale the numeric columns while keeping column names
extended[, numeric_columns] <- scale(extended[, numeric_columns])

extended_scaled <- extended %>%
  mutate_if(is.numeric, scale)
```

## Fit Models

Fit first model, with only the state as a random effect:

```{r extended_fit1}
fit_var_1 <- glmer(repay_fail ~ int_rate + emp_length + annual_inc + purpose +
                     inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate +
                     (1|addr_state), data = extended_scaled, family = binomial)
```

View first model summary:

```{r}
summary(fit_var_1)
```

Next, we'll add in another random effect of time, which is included as the variable "earliest_cr_line":

```{r}
fit_var_2 <- glmer(repay_fail ~ int_rate + emp_length + annual_inc + purpose +
                     inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate +
                     (1|addr_state) + (1|earliest_cr_line), data = extended_scaled, family = binomial)
```

Adding in time as a random effect caused a failed to converge warning. Thus, we will now increase the tolerance of this model to see if this helps:

```{r}
fit_var_3 <- glmer(repay_fail ~ int_rate + emp_length + annual_inc + purpose +
                     inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate +
                     (1|addr_state) + (1|earliest_cr_line), data = extended_scaled, family = binomial,
                   control = glmerControl(check.conv.grad = .makeCC("warning", tol = 3e-3, relTol = NULL)))
```

The model ran successfully, so we'll take a look at the summary:

```{r}
summary(fit_var_3)
```

Finally, we'll add in one more random effect, which is zip code. Once again, we will fit this model with an increased tolerance, compared to the default tolerance:

```{r}
fit_var_4 <- glmer(repay_fail ~ int_rate + emp_length + annual_inc + purpose +
                     inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate +
                     (1|addr_state) + (1|earliest_cr_line) + (1|zip_code),
                   data = extended_scaled, family = binomial,
                   control = glmerControl(check.conv.grad = .makeCC("warning", tol = 9e-3, relTol = NULL)))
```

Now we can view this model summary:

```{r}
summary(fit_var_4)
```

As the tolerance level for the above model was a bit high, we are now going to try to tune the optimizer to see if this allows for a ower tolerance.

```{r}
fit_var_5 <- glmer(repay_fail ~ int_rate + emp_length + annual_inc + purpose +
                     inq_last_6mths + pub_rec + revol_bal + revol_util + term * int_rate +
                     (1|addr_state) + (1|earliest_cr_line) + (1|zip_code),
                   data = extended_scaled, family = binomial,
                   control = glmerControl(optimizer = "Nelder_Mead",
                                          check.conv.grad = .makeCC("warning", tol = 5e-3, relTol = NULL)))
```


*** EXPLAIN

```{r runing_options - will be deleted}
# tuning options
# fit_var_2 <- update(fit_var_1, control = glmerControl(optimizer = "Nelder_Mead"))
# fit_var_3 <- update(fit_var_1, control = glmerControl(optimizer = "Nelder_Mead", tolPwrss = 1e-4))
# fit_var_4 <- update(fit_var_2, control = glmerControl(optCtrl=list(maxfun=2e4)))
```


## Goodness-of-Fit


## Analyse Performance Benchmarks with Extended Model


## Part 2 Discussion

...

# Conclusion

...

## Limitations

* Time frame of data model was built on?

## Future Directions

* Might want to explore other variables and interactions?

# References

Listed below are the references we consulted to conduct this analysis.

[Control of Mixed Model Fitting](https://search.r-project.org/CRAN/refmans/lme4/html/lmerControl.html)

[Five Cs of Credit](https://www.investopedia.com/ask/answers/040115/what-most-important-c-five-cs-credit.asp)

[lme4 Convergence Warnings: Troubleshooting](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html)

[lme4 Performance Tips](https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html#:~:text=choice%20of%20optimizer,may%20be%20worth%20a%20try.)

